<h1>Überwachtes Lernen mit scikit-learn</h1>
<h1>Beispiel: Iris-Klassifizierung un in scikit-learn</h1>
<!-- duplicate section in machine-learning-theory and scikit-learn -->
<h2>Überwachtes Lernen in scikit-learn</h2>
<p>Schritte:</p>
<ul>
<li>Erstellen einer Eingangsmatrix <code>X</code> und eines Zielvektors <code>y</code> / einer Zielmatrix <code>Y</code></li>
<li>Instanziierung einer Algorithmenklasse, z.B. <code>KNeighborsClassifier</code>, <code>MLPClassifier</code>, <code>LinearRegression</code>, ...</li>
<li>"Lernen" mittels <code>model.fit(X, y)</code></li>
<li>Voraussagen weiterer Ergebnisse mittels <code>model.predict(...)</code></li>
</ul>
<h2>Beispiel</h2>
<p>Beispiel: Klassifizierung von Iris-Pflanzen</p>
<p>Bekannte Daten: Maße und Klassifizierung von 150 Iris-Pflanzen (Schwertlilien)</p>
<p>Aufgabe: Trainieren eines Algorithmus, der anhand der Maße einer Iris-Pflanze eine Klassifizierung vornehmen kann</p>
<h2>Beispiel</h2>
<p>Beispieldaten (<em>sepal length</em>, <em>sepal width</em>, <em>petal length</em>, <em>petal width</em>, <em>name</em>):</p>
<ul>
<li><code>[5.1, 3.5, 1.4, 0.2]</code> → <code>"Iris-setosa"</code></li>
<li><code>[7.0, 3.2, 4.7, 1.4]</code> → <code>"Iris-versicolor"</code></li>
<li><code>[6.3, 3.3, 6.0, 2.5]</code> → <code>"Iris-virginica"</code></li>
</ul>
<p>in unseren Daten: <em>setosa</em>=0, <em>versicolor</em>=1, <em>virginica</em>=2</p>
<h2>Beispiel</h2>
<p>Vorbereiten der Daten:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets

iris = datasets.load_iris()

X = iris.data
y = iris.target
</code></pre>
<h2>Beispiel</h2>
<p>Trainieren eines Algorithmus:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier

model = KNeighborsClassifier()
model.fit(X, y)
</code></pre>
<h2>Beispiel</h2>
<p>Anwenden des Erlernten auf neue Daten:</p>
<pre><code class="hljs language-py">test_data = [
    [<span class="hljs-number">5.3</span>, <span class="hljs-number">3.4</span>, <span class="hljs-number">1.9</span>, <span class="hljs-number">0.6</span>],
    [<span class="hljs-number">6.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.7</span>, <span class="hljs-number">1.5</span>],
    [<span class="hljs-number">6.5</span>, <span class="hljs-number">3.1</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">1.7</span>]
]

y_pred = model.predict(test_data)
<span class="hljs-comment"># [0, 1, 1]</span>

y_pred_proba = model.predict_proba(test_data)
<span class="hljs-comment"># [[1.  0.  0. ]</span>
<span class="hljs-comment">#  [0.  0.8 0.2]</span>
<span class="hljs-comment">#  [0.  0.6 0.4]]</span>
</code></pre>
<h2>Beispiel</h2>
<p>Aufgabe: Verwenden anderer Klassifikatoren, z.B.:</p>
<ul>
<li><code>sklearn.neural_network.MLPClassifier</code></li>
<li><code>sklearn.svm.SVC</code></li>
<li><code>sklearn.tree.DecisionTreeClassifier</code></li>
<li><code>sklearn.naive_bayes.GaussianNB</code></li>
</ul>
<h1>Daten vorbereiten</h1>
<h2>Daten vorbereiten</h2>
<p>erwünschtes Datenformat für Machine Learning Algorithmen:</p>
<ul>
<li><em>x</em> oder <em>X</em>: zweidimensionales Array mit numerischen Eingangsdaten</li>
<li><em>y</em> oder <em>Y</em>: ein- oder zweidimensionales Array mit numerischen Resultaten</li>
</ul>
<h2>Daten vorbereiten</h2>
<p>Aufgaben:</p>
<ul>
<li>"Flattening" von verschachtelten Daten</li>
<li>Skalieren von Werten</li>
<li>Fehlende Daten ergänzen</li>
<li>Kategoriedaten in numerische Daten umwandeln</li>
<li>Textdaten in numerische Daten umwandeln</li>
</ul>
<h2>Daten vorbereiten in sklearn</h2>
<p>Klassen zum vorbereiten der Daten besitzen folgende Methoden:</p>
<ul>
<li><code>.fit</code>: erlernt anhand vorgegebener Eingangsdaten (<code>X1</code>) eine passende Umwandlung für das Modell</li>
<li><code>.transform</code>: wandelt gegebene Eingangsdaten (<code>X2</code>) anhand des gelernten in die neue Form um</li>
<li><code>.fit_transform</code>: beides in einem Schritt (für die gleichen Daten)</li>
<li><code>.inverse_transform</code>: umgekehrte Transformation (nicht für alle Transformationen vorhanden)</li>
</ul>
<h2>Skalieren von Werten</h2>
<p>Zentrieren und Skalieren der Werte, sodass ihr Mittelwert 0 und ihre Standardabweichung 1 ist:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

stars = np.array([[ <span class="hljs-number">7.0e7</span>, <span class="hljs-number">2.0e30</span>, <span class="hljs-number">5.8e3</span>],
                  [ <span class="hljs-number">6.5e7</span>, <span class="hljs-number">2.2e30</span>, <span class="hljs-number">5.2e3</span>],
                  [ <span class="hljs-number">7.0e9</span>, <span class="hljs-number">2.1e30</span>, <span class="hljs-number">3.1e3</span>]])

scaler = preprocessing.StandardScaler().fit(stars)
X = scaler.transform(stars)
</code></pre>
<h2>Skalieren von Werten</h2>
<p>Skalierte Werte:</p>
<pre><code class="hljs language-py">array([[-<span class="hljs-number">0.70634165</span>, -<span class="hljs-number">1.22474487</span>,  <span class="hljs-number">0.95025527</span>],
       [-<span class="hljs-number">0.70787163</span>,  <span class="hljs-number">1.22474487</span>,  <span class="hljs-number">0.43193421</span>],
       [ <span class="hljs-number">1.41421329</span>,  <span class="hljs-number">0.</span>        , -<span class="hljs-number">1.38218948</span>]])
</code></pre>
<h2>Fehlende Daten</h2>
<p>Interpolation:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.impute <span class="hljs-keyword">import</span> SimpleImputer

X = np.array([[ np.nan, <span class="hljs-number">0</span>,   <span class="hljs-number">3</span>  ],
              [ <span class="hljs-number">3</span>,   <span class="hljs-number">7</span>,   <span class="hljs-number">9</span>  ],
              [ <span class="hljs-number">3</span>,   <span class="hljs-number">5</span>,   <span class="hljs-number">2</span>  ],
              [ <span class="hljs-number">4</span>,   np.nan, <span class="hljs-number">6</span>  ],
              [ <span class="hljs-number">8</span>,   <span class="hljs-number">8</span>,   <span class="hljs-number">1</span>  ]])

imputer = SimpleImputer(strategy=<span class="hljs-string">"mean"</span>).fit(X)

imputer.transform(X)
imputer.transform(np.array([[np.nan, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]))
</code></pre>
<h2>Kategorien als Daten</h2>
<p>Preprocessors:</p>
<ul>
<li><code>OrdinalEncoder</code> (Ordinale für Eingangskategorien)</li>
<li><code>LabelEncoder</code> (Ordinale für Zielkategorien)</li>
<li><code>OneHotEncoder</code> (One-Hot-Encoding für Eingangskategorien, standardmäßig sparse)</li>
<li><code>LabelBinarizer</code> (One-Hot-Encoding für Zielkategorien)</li>
</ul>
<h2>Kategorien als Daten</h2>
<p>Beispiel:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelBinarizer

encoder = LabelBinarizer().fit(iris_species)
iris_species_one_hot = encoder.transform(iris_species)
</code></pre>
<h2>Textdaten</h2>
<p>Beispiel für das Preprocessing von Textdaten: Zählen von Worten</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer

sample = [<span class="hljs-string">'problem of evil'</span>,
          <span class="hljs-string">'evil queen'</span>,
          <span class="hljs-string">'horizon problem'</span>]

vectorizer = CountVectorizer().fit(sample)
print(vectorizer.vocabulary_)
X = vectorizer.transform(sample)
print(X)
print(X.todense())
</code></pre>
<h2>Aufgabe: Vorbereiten von Iris-Rohdaten</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
iris = pd.read_csv(
    <span class="hljs-string">"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"</span>,
    header=<span class="hljs-literal">None</span>)
</code></pre>
<p>erste Zeile: <code>5.1,3.5,1.4,0.2,Iris-setosa</code></p>
<p>Aufgaben:</p>
<ul>
<li>Zieldaten als ordinale Daten oder mittels one-hot-Encoding</li>
<li>Eingangsdaten skalieren</li>
<li>k-Nearest-Neighbor-Klassifizierung bei skalierten und nichtskalierten Daten vergleichen</li>
</ul>
<h1>Pipelines</h1>
<h2>Pipelines</h2>
<p>Pipelines können aus mehreren transformierenden Algorithmen und einem vorhersagenden Algorithmus zusammengesetzt werden:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> make_pipeline
<span class="hljs-keyword">from</span> sklearn.impute <span class="hljs-keyword">import</span> SimpleImputer
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression

model = make_pipeline(
    SimpleImputer(strategy=<span class="hljs-string">'mean'</span>),
    StandardScaler(),
    LinearRegression()
)
</code></pre>
<h2>Pipelines</h2>
<p>Aufgabe:</p>
<p>Erstelle eine Pipeline für die Kategorisierung von Iris-Daten</p>
<h1>Speichern und Laden von Modellen</h1>
<h2>Speichern und Laden von Modellen</h2>
<p>Ein trainiertes Modell kann für spätere Verwendung gespeichert werden</p>
<p>In Python können Objekte mittels des <code>pickle</code>-Moduls gespeichert / geladen werden</p>
<h2>Speichern von Modellen</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">import</span> pickle

<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">"model.pickle"</span>, mode=<span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> picklefile:
    pickle.dump(model, picklefile)
</code></pre>
<h2>Laden von Modellen</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">import</span> pickle

<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">"model.pickle"</span>, mode=<span class="hljs-string">"rb"</span>) <span class="hljs-keyword">as</span> picklefile:
    model = pickle.load(picklefile)

model.predict(data)
</code></pre>
<h1>Supervised Learning Algorithmen in scikit-learn</h1>
<h2>Algorithmen in scikit-learn</h2>
<p>Regression:</p>
<ul>
<li><code>sklearn.linear_model.LinearRegression</code></li>
<li><code>sklearn.neural_network.MLPRegressor</code></li>
</ul>
<p>Klassifizierung:</p>
<ul>
<li><code>sklearn.neighbors.KNeighborsClassifier</code></li>
<li><code>sklearn.naive_bayes.GaussianNB</code></li>
<li><code>sklearn.naive_bayes.MultinomialNB</code></li>
<li><code>sklearn.linear_model.LogisticRegression</code></li>
<li><code>sklearn.svm.SVC</code></li>
<li><code>sklearn.tree.DecisionTreeClassifier</code></li>
<li><code>sklearn.ensemble.RandomForestClassifier</code></li>
<li><code>sklearn.neural_network.MLPClassifier</code></li>
</ul>
<h2>K-Nearest-Neighbors</h2>
<p><code>sklearn.neighbors.KNeighborsClassifier</code></p>
<p>Die Anzahl <code>k</code> der betrachteten Nachbarn kann festgesetzt werden (Standardwert = 5)</p>
<p>Siehe auch: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier</a></p>
<h2>Logistische Regression</h2>
<p>Beispiel: <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py">https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py</a></p>
<pre><code class="hljs language-py">LogisticRegression(solver=<span class="hljs-string">"liblinear"</span>, multi_class=<span class="hljs-string">"auto"</span>)
</code></pre>
<h2>Naive Bayes</h2>
<p>siehe: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html">Python Data Science Handbook - Naive Bayes</a></p>
<h2>Support Vector Machines</h2>
<p>siehe:</p>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a></li>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html">Python Data Science Handbook - Support Vector Machines</a></li>
</ul>
<h2>Entscheidungsbäume (Decision Trees)</h2>
<p>siehe: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html">Python Data Science Handbook - Decision Trees and Random Forests</a></p>
<p>Random Forests: Die Daten werden in verschiedene Untermengen zerlegt. Mittels jeder Untermenge wird ein einzelner Decision Tree erstellt. Die Gesamtheit der Decision Trees wird zu einem sogenannten <em>Random Forest</em> zusammengeführt.</p>
<pre><code class="hljs language-py">RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>)
</code></pre>
<h2>Beispiele</h2>
<ul>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html#Multinomial-Naive-Bayes">Klassifizierung von Newsgroup-Postings (mittels Naive Bayes, logistischer Regression oder Decision Tree)</a></li>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html#Example:-Random-Forest-for-Classifying-Digits">Erkennen von Ziffern (Random Forest)</a></li>
</ul>
<h1>Lineare Regression mit scikit-learn</h1>
<h2>Lineare Regression mit scikit-learn</h2>
<p>Beispiel: verschiedene Einkäufe bei verschiedenen Supermärkten:</p>
<ul>
<li>1 l Milch, 1 kg Brot: 5.00€</li>
<li>2 l Milch, 3 kg Brot: 13.50€</li>
<li>3 l Milch, 2 kg Brot: 10.90€</li>
<li>(0 l Milch, 0 kg Brot: 0€)</li>
</ul>
<h2>Lineare Regression mit scikit-learn</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression

X = [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]
y = [<span class="hljs-number">4.60</span>, <span class="hljs-number">14.50</span>, <span class="hljs-number">12.00</span>, <span class="hljs-number">0.0</span>]

model = LinearRegression()
model.fit(X, y)

yfit = model.predict([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]])
print(yfit)
<span class="hljs-comment"># [1.18333333 3.78333333 9.78333333]</span>
</code></pre>
<h2>Lineare Regression mit scikit-learn</h2>
<p>Kennzahlen der "erlernten" Regression:</p>
<ul>
<li><code>model.coef_</code></li>
<li><code>model.intercept_</code></li>
</ul>
<h2>Lineare Regression mit scikit-learn</h2>
<p>Übung: Abschätzen der <em>petal width</em> (Spaltenindex 3) basierend auf der <em>petal length</em> (Spaltenindex 2) bei den Iris-Daten</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
iris = datasets.load_iris()
</code></pre>
<h2>Beispiele</h2>
<ul>
<li>Diabetes Vorhersage</li>
<li>(<a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic">Radverkehr</a>)</li>
</ul>
<h1>Polynomiale Regression mit scikit-learn</h1>
<h2>Polynomiale Regression</h2>
<p>Manche Daten passen nicht in das Schema eines linearen Zusammenhangs wie:</p>
<p><code>y = a*x + b</code>.</p>
<p>Wir könnten einen polynomialen Zusammenhang annehmen, z.B.:</p>
<p><code>y = a*x^2 + b*x + c</code></p>
<p><code>y = a*x^3 + b*x^2 + c*x + d</code></p>
<h2>Polynomiale Regression mit scikit-learn</h2>
<p>scikit-learn bietet einen <em>Preprocessor</em> namens <code>PolynomialFeatures</code>.</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures

poly_model = make_pipeline(
    PolynomialFeatures(<span class="hljs-number">2</span>),
    LinearRegression()
)

poly_model.fit(x, y)
</code></pre>
<h2>Übungen</h2>
<ul>
<li>verwende eine polynomiale Regression anstatt einer linearen Regression für eines der bisherigen Beispiele</li>
<li>verwende eine polynomiale Regression, um den Datensatz <em>II</em> der sogenannten Anscombe-Daten zu approximieren (kann mittels der <em>seaborn</em>-Library geladen werden)</li>
</ul>
<h1>Regression mittels neuronalem Netzwerk</h1>
<h2>Regression mittels neuronalem Netzwerk</h2>
<p>Iris-Datensatz: Abschätzen der Spalte 0 basierend auf Spalten 1 und 2</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPRegressor

iris = datasets.load_iris()

X = iris.data[:,<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]
y = iris.data[:, <span class="hljs-number">0</span>]

model = MLPRegressor(
    hidden_layer_sizes=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>),
    alpha=<span class="hljs-number">1.0</span>,
    max_iter=<span class="hljs-number">2000</span>
)
model.fit(X, y)
</code></pre>
<h2>Regression mittels neuronalem Netzwerk</h2>
<pre><code class="hljs language-py">test_data = [
    [<span class="hljs-number">3.4</span>, <span class="hljs-number">1.9</span>],
    [<span class="hljs-number">3.0</span>, <span class="hljs-number">4.7</span>],
    [<span class="hljs-number">3.1</span>, <span class="hljs-number">5.0</span>]
]

y_pred = model.predict(test_data)
print(y_pred)
</code></pre>
<h1>Validierung</h1>
<h2>Train-Test Split</h2>
<p>Wie gut kategorisiert ein bestimmter Algorithmus die Iris-Daten?</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics

X_train, X_test, Y_train, Y_test = train_test_split(X, Y)

<span class="hljs-comment"># ...</span>

Y_prediction = model.predict(X_test)
print(metrics.accuracy_score(Y_test, Y_prediction))
</code></pre>
<p>optionaler Parameter: <code>test_size</code> (Standardwert <code>0.25</code>)</p>
<h2>Kreuzvalidierung</h2>
<p>Bei der Kreuzvalidierung (cross-validation) werden die Daten wiederholt in unterschiedliche Trainings- und Testdaten unterteilt, sodass jeder Eintrag einmal in den Testdaten vorkommt.</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_validate

test_results = cross_validate(
    model, X, y, cv=<span class="hljs-number">5</span>, scoring=<span class="hljs-string">"accuracy"</span>
)
print(test_results[<span class="hljs-string">"test_score"</span>])
</code></pre>
<h2>Beispiel: ROC der Iris-Daten</h2>
<p>Bestimmen der ROC mit scikit-learn:</p>
<pre><code class="hljs language-py">false_positive_rates, true_positive_rates, thresholds = metrics.roc_curve(
    y_test,
    classifier.predict_proba(X_test)[: <span class="hljs-number">1</span>]
)
</code></pre>
<p>ideale Kombination: <em>false positive rate</em> = 0, <em>true positive rate</em> = 1</p>
<h2>Beispiel: ROC der Iris-Daten</h2>
<p>Zeichnen der ROC:</p>
<pre><code class="hljs language-py">plt.plot(false_positive_rate, true_positive_rate)
</code></pre>
<p>Bestimmen der AUC:</p>
<pre><code class="hljs language-py">auc = metrics.auc(false_positive_rates, true_positive_rates)
</code></pre>
<h1>Example: Erkennung von Ziffern</h1>
<h2>Example: Erkennung von Ziffern</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets, model_selection, metrics

digits = datasets.load_digits()
<span class="hljs-comment"># flatten array from 1797x8x8 to 1797x64</span>
X = digits.images.reshape(digits.images.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)
y = digits.target

X_train, X_test, y_train, y_test =
    model_selection.train_test_split(X, y)

model = KNeighborsClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(metrics.accuracy_score(y_test, y_pred))
</code></pre>
<h1>Beispiel: Gesichtserkennung mit scikit-learn</h1>
<h2>Beispiel: Gesichtserkennung</h2>
<p><a href="http://vis-www.cs.umass.edu/lfw/number_11.html">Beispieldaten</a></p>
<p>Eingangsdaten: Schwarz-weiß-Bilder bekannter Personen (Größe 62 x 47) und deren Namen</p>
<p>Ziel: erkennen der Personen mittels eines neuronalen Netzwerks</p>
<h2>Daten laden</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=<span class="hljs-number">60</span>)
</code></pre>
<p>Einträge:</p>
<ul>
<li><code>faces.images</code>: Array von Bildern (1248 x 62 x 47)</li>
<li><code>faces.target</code>: Array von numerischen Labeln (1, 3, 3, 3, 5, ...)</li>
<li><code>faces.target_names</code>: Array von Labelnamen (0="Ariel Sharon", 1="Colin Powell", ...)</li>
</ul>
<h2>Daten vorbereiten</h2>
<pre><code class="hljs language-py">num_images = faces.images.shape[<span class="hljs-number">0</span>]
num_pixels = faces.images.shape[<span class="hljs-number">1</span>] * faces.images.shape[<span class="hljs-number">2</span>]
X = faces.images.reshape(num_images, num_pixels)

<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelBinarizer
encoder = LabelBinarizer().fit(faces.target)
Y = encoder.transform(faces.target)
</code></pre>
<h2>Train-Test Split</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y)
</code></pre>
<h2>Erstellen und Trainieren eines Klassifikators</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier

model = MLPClassifier(hidden_layer_sizes=(<span class="hljs-number">250</span>, <span class="hljs-number">150</span>, <span class="hljs-number">100</span>),
                      early_stopping=<span class="hljs-literal">True</span>,
                      n_iter_no_change=<span class="hljs-number">100</span>,
                      max_iter=<span class="hljs-number">2000</span>,
                      verbose=<span class="hljs-literal">True</span>)
model.fit(X_train, Y_train)
</code></pre>
<p>Konfiguration des Algorithmus:</p>
<ul>
<li>drei Schichten an Neuronen mit je 250, 150 und 100 Neuronen</li>
<li>Algorithmus stoppt, wenn für die letzten 100 Iterationsschritte keine Verbesserung eintrat</li>
<li>Algorithmus stoppt nach maximal 2000 Iterationen</li>
</ul>
<h2>Testen</h2>
<pre><code class="hljs language-py"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics

real_labels = Y_test.argmax(axis=<span class="hljs-number">1</span>)
pred_labels = model.predict_proba(X_test).argmax(axis=<span class="hljs-number">1</span>)

print(metrics.accuracy_score(real_labels, pred_labels))
</code></pre>
<p><code>argmax</code> gibt den index des größten Eintrags im Array zurück</p>
<h2>Testen</h2>
<p>Anzeigen eines zufälligen Gesichts, des echten Namens und des vorhergesagten Namens:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> randrange

<span class="hljs-comment"># randomly select a face</span>
index = randrange(X_test.shape[<span class="hljs-number">0</span>])

plt.imshow(X_test[index].reshape(<span class="hljs-number">62</span>, <span class="hljs-number">47</span>), cmap=<span class="hljs-string">"gray"</span>)

real_label = real_labels[index]
pred_label = pred_labels[index]

print(<span class="hljs-string">"real name:"</span>, faces.target_names[real_label])
print(<span class="hljs-string">"predicted name:"</span>, faces.target_names[pred_label])
</code></pre>
<h1>Abstraktion</h1>
<h2>Abstraktion</h2>
<ul>
<li>Pipelines</li>
<li>eigene Klassen</li>
</ul>
<h2>Abstraktion</h2>
<p><em>Pipelines</em> können das Verarbeiten von Eingangswerten <em>x</em> abstrahieren</p>
<p>Eigene Klassen können das Verarbeiten von sowohl <em>x</em> als auch <em>y</em> abstrahieren</p>
<h2>Abstraktion</h2>
<p>direkte Verwendung eines Modells, um Überleben auf der Titanic vorherzusagen:</p>
<pre><code class="hljs language-py">model.predict([[<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">28.0</span>, <span class="hljs-number">0</span>]])
<span class="hljs-comment"># [0]</span>
</code></pre>
<p>abstrahiertes Interface:</p>
<pre><code class="hljs language-py">classifier.predict_survival(
    pclass=<span class="hljs-number">2</span>, sex=<span class="hljs-string">"male"</span>, age=<span class="hljs-number">28.0</span>, sibsp=<span class="hljs-number">0</span>
)
<span class="hljs-comment"># False</span>
</code></pre>